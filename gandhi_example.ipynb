{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13885c46",
   "metadata": {
    "papermill": {
     "duration": 8.967077,
     "end_time": "2025-06-18T04:39:07.515890",
     "exception": false,
     "start_time": "2025-06-18T04:38:58.548813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166376d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada41e68-61b3-4d3c-aa2f-ff36a455bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNorm(nn.Module):\n",
    "    \"\"\"Spectral Normalization\"\"\"\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = F.normalize(torch.mv(torch.t(w.view(height, -1).data), u.data), dim=0)\n",
    "            u.data = F.normalize(torch.mv(w.view(height, -1).data, v.data), dim=0)\n",
    "\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = F.normalize(u.data, dim=0)\n",
    "        v.data = F.normalize(v.data, dim=0)\n",
    "        w_bar = nn.Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "def spectral_norm(module, name='weight', power_iterations=1):\n",
    "    return SpectralNorm(module, name, power_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2f57-924e-4e18-8399-411a47a0a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encode MNIST images (28x28 grayscale) to latent features \"\"\"\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 14x14\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 7x7\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))  # 4x4\n",
    "        )\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 1, 28, 28)\n",
    "        features = self.conv_layers(x)\n",
    "        features = features.view(features.size(0), -1)  \n",
    "        projected = self.projection_head(features)\n",
    "        return F.normalize(projected, dim=1)  # L2 normalize for contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de34784-ab45-4377-8cf6-f3cb6571fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtviz ImageNet and eggimagedecode\n",
    "\n",
    "# class ImageEncoder(nn.Module):\n",
    "#     \"\"\"Encode images to latent features for contrastive learning\"\"\"\n",
    "#     def __init__(self, latent_dim=512):\n",
    "#         super(ImageEncoder, self).__init__()\n",
    "        \n",
    "#         resnet = resnet50(pretrained=True)\n",
    "#         self.backbone = nn.Sequential(*list(resnet.children())[:-1])  \n",
    "        \n",
    "#         self.projection_head = nn.Sequential(\n",
    "#             nn.Linear(2048, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, latent_dim)\n",
    "#         )\n",
    "        \n",
    "#         for param in list(self.backbone.parameters())[:-20]:\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, 3, 128, 128)\n",
    "#         features = self.backbone(x)\n",
    "#         features = features.view(features.size(0), -1)  \n",
    "#         projected = self.projection_head(features)\n",
    "#         return F.normalize(projected, dim=1)  # L2 normalize for contrastive learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7537f0-35ee-4505-8d46-12ebca9663e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEncoder(nn.Module):\n",
    "    \"\"\"EEG Encoder for shape (batch, 14, 32, 1)\"\"\"\n",
    "    def __init__(self, input_channels=14, sequence_length=32, latent_dim=512):\n",
    "        super(EEGEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(256, 128, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * sequence_length, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, latent_dim)\n",
    "        \n",
    "        self.contrastive_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x, return_contrastive=False):\n",
    "        # x shape: (batch_size, 14, 32, 1) -> (batch_size, 14, 32)\n",
    "        x = x.squeeze(-1)  \n",
    "        \n",
    "        x = self.leaky_relu(self.conv1(x))  # (batch, 64, 32)\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.conv2(x))  # (batch, 128, 32)\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.conv3(x))  # (batch, 256, 32)\n",
    "        \n",
    "        # Transpose for LSTM: (batch, seq_len, features)\n",
    "        x = x.transpose(1, 2)  # (batch, 32, 256)\n",
    "        \n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        x = x.flatten(1)\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        features = self.fc3(x)\n",
    "        \n",
    "        if return_contrastive:\n",
    "            contrastive_features = self.contrastive_head(features)\n",
    "            contrastive_features = F.normalize(contrastive_features, dim=1)\n",
    "            return features, contrastive_features\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f3ec6-1819-4a6c-96f9-200d4b122b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eggimagedecode\n",
    "\n",
    "# class EEGEncoder(nn.Module):\n",
    "#     \"\"\"Encode EEG signals to latent features\"\"\"\n",
    "#     def __init__(self, input_channels=17, sequence_length=100, latent_dim=512):\n",
    "#         super(EEGEncoder, self).__init__()\n",
    "        \n",
    "#         self.lstm1 = nn.LSTM(input_channels, 128, batch_first=True, bidirectional=True)\n",
    "#         self.lstm2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(128 * sequence_length, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.fc3 = nn.Linear(512, latent_dim)\n",
    "        \n",
    "#         self.contrastive_head = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, latent_dim)\n",
    "#         )\n",
    "        \n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "#     def forward(self, x, return_contrastive=False):\n",
    "#         # x shape: (batch_size, 4, 17, 100) -> (batch_size, 100, 17)\n",
    "#         x = x.mean(dim=1)  # Average across 4 trials\n",
    "#         x = x.permute(0, 2, 1)  # (batch_size, 100, 17)\n",
    "        \n",
    "#         x, _ = self.lstm1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x, _ = self.lstm2(x)\n",
    "        \n",
    "#         x = x.flatten(1)\n",
    "#         x = self.leaky_relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.leaky_relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         features = self.fc3(x)\n",
    "        \n",
    "#         if return_contrastive:\n",
    "#             contrastive_features = self.contrastive_head(features)\n",
    "#             contrastive_features = F.normalize(contrastive_features, dim=1)\n",
    "#             return features, contrastive_features\n",
    "        \n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8422192-983d-4664-ac50-8783b73e95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator for MNIST - outputs 28x28 grayscale images\"\"\"\n",
    "    def __init__(self, eeg_dim=512, noise_dim=100, n_classes=10):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.class_embedding = nn.Embedding(n_classes, 50)\n",
    "        \n",
    "        input_dim = eeg_dim + noise_dim + 50\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, 7 * 7 * 256)\n",
    "        \n",
    "        self.convt1 = spectral_norm(nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False))\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.convt2 = spectral_norm(nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.convt3 = spectral_norm(nn.ConvTranspose2d(64, 1, 3, 1, 1, bias=False))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, eeg_features, noise, class_labels):\n",
    "        class_emb = self.class_embedding(class_labels)\n",
    "        \n",
    "        x = torch.cat([eeg_features, noise, class_emb], dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 7, 7)\n",
    "        \n",
    "        x = self.leaky_relu(self.bn1(self.convt1(x)))  # 14x14\n",
    "        x = self.leaky_relu(self.bn2(self.convt2(x)))  # 28x28\n",
    "        x = self.tanh(self.convt3(x))                   # 28x28x1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b791c1-d8ac-41e4-a656-220c5c3ee315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtviz ImageNet\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     \"\"\"Generator for 10 classes\"\"\"\n",
    "#     def __init__(self, eeg_dim=512, noise_dim=100, n_classes=10): # n_classes=1654 for eggimagedecode\n",
    "#         super(Generator, self).__init__()\n",
    "        \n",
    "#         self.class_embedding = nn.Embedding(n_classes, 50)\n",
    "        \n",
    "#         input_dim = eeg_dim + noise_dim + 50\n",
    "        \n",
    "#         self.fc = nn.Linear(input_dim, 8 * 8 * 1024)\n",
    "        \n",
    "#         self.convt1 = spectral_norm(nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False))\n",
    "#         self.bn1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "#         self.convt2 = spectral_norm(nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False))\n",
    "#         self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "#         self.convt3 = spectral_norm(nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False))\n",
    "#         self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         self.convt4 = spectral_norm(nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False))\n",
    "#         self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "#         self.convt5 = spectral_norm(nn.ConvTranspose2d(64, 3, 3, 1, 1, bias=False))\n",
    "        \n",
    "#         self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "#         self.tanh = nn.Tanh()\n",
    "        \n",
    "#     def forward(self, eeg_features, noise, class_labels):\n",
    "#         class_emb = self.class_embedding(class_labels)\n",
    "        \n",
    "#         x = torch.cat([eeg_features, noise, class_emb], dim=1)\n",
    "        \n",
    "#         x = self.fc(x)\n",
    "#         x = x.view(-1, 1024, 8, 8)\n",
    "        \n",
    "#         x = self.leaky_relu(self.bn1(self.convt1(x)))  # 16x16\n",
    "#         x = self.leaky_relu(self.bn2(self.convt2(x)))  # 32x32\n",
    "#         x = self.leaky_relu(self.bn3(self.convt3(x)))  # 64x64\n",
    "#         x = self.leaky_relu(self.bn4(self.convt4(x)))  # 128x128\n",
    "#         x = self.tanh(self.convt5(x))                   # 128x128x3\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effc022-403b-440e-9890-6cd158e7a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Discriminator for MNIST - processes 28x28 grayscale images\"\"\"\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.class_embedding = nn.Embedding(n_classes, 50)\n",
    "        self.class_projection = nn.Linear(50, 28 * 28)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 64, 3, 2, 1, bias=False)  # 14x14\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 2, 1, bias=False)  # 7x7\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 2, 1, bias=False)  # 3x3\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 1, 3, 1, 1, bias=False)  # 3x3\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x, class_labels):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        class_emb = self.class_embedding(class_labels)\n",
    "        class_map = self.class_projection(class_emb)\n",
    "        class_map = class_map.view(batch_size, 1, 28, 28)\n",
    "        \n",
    "        x = torch.cat([x, class_map], dim=1)\n",
    "        \n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        return x.view(batch_size, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0b2b8-2e97-44f8-8073-dc0f0a92f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtviz ImageNet\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     \"\"\"Discriminator for 10 classes\"\"\"\n",
    "#     def __init__(self, n_classes=10): # n_classes=1654 for eggimagedecode\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         self.class_embedding = nn.Embedding(n_classes, 50)\n",
    "#         self.class_projection = nn.Linear(50, 128 * 128)\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(4, 64, 3, 2, 1, bias=False)  # 4 channels (3 RGB + 1 class)\n",
    "#         self.conv2 = nn.Conv2d(64, 128, 3, 2, 1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(128, 256, 3, 2, 1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(256, 512, 3, 2, 1, bias=False)\n",
    "#         self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(512, 1024, 3, 1, 1, bias=False)\n",
    "#         self.bn5 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "#         self.conv6 = nn.Conv2d(1024, 1, 3, 1, 1, bias=False)\n",
    "        \n",
    "#         self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "#     def forward(self, x, class_labels):\n",
    "#         batch_size = x.size(0)\n",
    "        \n",
    "#         class_emb = self.class_embedding(class_labels)\n",
    "#         class_map = self.class_projection(class_emb)\n",
    "#         class_map = class_map.view(batch_size, 1, 128, 128)\n",
    "        \n",
    "#         x = torch.cat([x, class_map], dim=1)\n",
    "        \n",
    "#         x = self.leaky_relu(self.conv1(x))\n",
    "#         x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "#         x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "#         x = self.leaky_relu(self.bn4(self.conv4(x)))\n",
    "#         x = self.leaky_relu(self.bn5(self.conv5(x)))\n",
    "#         x = self.conv6(x)\n",
    "        \n",
    "#         return x.view(batch_size, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade934a-8ac5-424c-9623-1f7c1bafb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(eeg_features, img_features, temperature=0.07):\n",
    "    \"\"\"\n",
    "    InfoNCE loss for contrastive learning\n",
    "    \"\"\"\n",
    "    batch_size = eeg_features.size(0)\n",
    "    \n",
    "    similarity_matrix = torch.matmul(eeg_features, img_features.T) / temperature\n",
    "    labels = torch.arange(batch_size).to(eeg_features.device)\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    \"\"\"\n",
    "    Triplet loss for contrastive learning\n",
    "    \"\"\"\n",
    "    pos_dist = F.pairwise_distance(anchor, positive)\n",
    "    neg_dist = F.pairwise_distance(anchor, negative)\n",
    "    \n",
    "    loss = F.relu(pos_dist - neg_dist + margin)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5cecc-f0b6-46eb-82d1-26f1fdabbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMNISTDataset(Dataset):\n",
    "    def __init__(self, eeg_pickle_path, train=True, transform=None):\n",
    "        with open(eeg_pickle_path, 'rb') as f:\n",
    "            data_dict = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        if train:\n",
    "            self.eeg_data = data_dict['x_train']  # (45390, 14, 32, 1)\n",
    "            self.eeg_labels = data_dict['y_train']  # (45390, 10) - one-hot encoded\n",
    "        else:\n",
    "            self.eeg_data = data_dict['x_test']\n",
    "            self.eeg_labels = data_dict['y_test']\n",
    "        \n",
    "        self.class_labels = np.argmax(self.eeg_labels, axis=1)\n",
    "        \n",
    "        from torchvision.datasets import MNIST\n",
    "        \n",
    "        mnist_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        self.mnist_dataset = MNIST(\n",
    "            root='./mnist_data', \n",
    "            train=train, \n",
    "            download=True, \n",
    "            transform=mnist_transform\n",
    "        )\n",
    "        \n",
    "        self.class_to_mnist_images = {i: [] for i in range(10)}\n",
    "        for idx, (_, label) in enumerate(self.mnist_dataset):\n",
    "            self.class_to_mnist_images[label].append(idx)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"Dataset: {len(self.eeg_data)} EEG samples, {len(self.mnist_dataset)} MNIST images\")\n",
    "        print(f\"EEG shape: {self.eeg_data.shape}\")\n",
    "        print(f\"EEG labels shape: {self.eeg_labels.shape}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eeg = torch.FloatTensor(self.eeg_data[idx])  # (14, 32, 1)\n",
    "        eeg_class = self.class_labels[idx]\n",
    "        \n",
    "        if len(self.class_to_mnist_images[eeg_class]) > 0:\n",
    "            mnist_idx = random.choice(self.class_to_mnist_images[eeg_class])\n",
    "        else:\n",
    "            mnist_idx = random.randint(0, len(self.mnist_dataset) - 1)\n",
    "            \n",
    "        mnist_image, mnist_label = self.mnist_dataset[mnist_idx]\n",
    "        \n",
    "        available_classes = [c for c in range(10) if c != mnist_label and len(self.class_to_mnist_images[c]) > 0]\n",
    "        if available_classes:\n",
    "            negative_class = random.choice(available_classes)\n",
    "            negative_mnist_idx = random.choice(self.class_to_mnist_images[negative_class])\n",
    "        else:\n",
    "            negative_mnist_idx = random.choice([i for i in range(len(self.mnist_dataset)) \n",
    "                                              if self.mnist_dataset[i][1] != mnist_label])\n",
    "        \n",
    "        negative_mnist_image, _ = self.mnist_dataset[negative_mnist_idx]\n",
    "        \n",
    "        return eeg, mnist_image, negative_mnist_image, torch.LongTensor([mnist_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf35350-2754-4f5a-b95e-9c0adefee431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtviz ImageNet\n",
    "\n",
    "# class EEGImageDataset(Dataset):\n",
    "#     def __init__(self, eeg_pickle_path, images_dir, transform=None):\n",
    "#         with open(eeg_pickle_path, 'rb') as f:\n",
    "#             data_dict = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "#         self.eeg_data = data_dict['x_train']  # (45390, 14, 32, 1)\n",
    "#         self.eeg_labels = data_dict['y_train']  # (45390, 10) - one-hot encoded\n",
    "        \n",
    "#         self.class_labels = np.argmax(self.eeg_labels, axis=1)\n",
    "        \n",
    "#         self.images_dir = Path(images_dir)\n",
    "#         self.class_dirs = sorted([d for d in self.images_dir.iterdir() if d.is_dir()])\n",
    "#         self.class_to_idx = {d.name: i for i, d in enumerate(self.class_dirs)}\n",
    "        \n",
    "#         self.image_paths = []\n",
    "#         self.image_labels = []\n",
    "        \n",
    "#         for class_idx, class_dir in enumerate(self.class_dirs):\n",
    "#             image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPEG')) + list(class_dir.glob('*.jpeg'))\n",
    "#             for img_path in image_files:\n",
    "#                 self.image_paths.append(img_path)\n",
    "#                 self.image_labels.append(class_idx)\n",
    "        \n",
    "#         self.class_to_images = {}\n",
    "#         for i, label in enumerate(self.image_labels):\n",
    "#             if label not in self.class_to_images:\n",
    "#                 self.class_to_images[label] = []\n",
    "#             self.class_to_images[label].append(i)\n",
    "        \n",
    "#         self.transform = transform or transforms.Compose([\n",
    "#             transforms.Resize((128, 128)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "#         ])\n",
    "        \n",
    "#         print(f\"Dataset: {len(self.eeg_data)} EEG samples, {len(self.image_paths)} images\")\n",
    "#         print(f\"Classes: {len(self.class_dirs)}\")\n",
    "#         print(f\"EEG shape: {self.eeg_data.shape}\")\n",
    "#         print(f\"EEG labels shape: {self.eeg_labels.shape}\")\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.eeg_data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         eeg = torch.FloatTensor(self.eeg_data[idx])  # (14, 32, 1)\n",
    "#         eeg_class = self.class_labels[idx]\n",
    "        \n",
    "#         if eeg_class in self.class_to_images and len(self.class_to_images[eeg_class]) > 0:\n",
    "#             img_idx = random.choice(self.class_to_images[eeg_class])\n",
    "#         else:\n",
    "#             img_idx = random.randint(0, len(self.image_paths) - 1)\n",
    "            \n",
    "#         img_path = self.image_paths[img_idx]\n",
    "#         img_class = self.image_labels[img_idx]\n",
    "        \n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "        \n",
    "#         available_classes = [c for c in self.class_to_images.keys() if c != img_class and len(self.class_to_images[c]) > 0]\n",
    "#         if available_classes:\n",
    "#             negative_class = random.choice(available_classes)\n",
    "#             negative_img_idx = random.choice(self.class_to_images[negative_class])\n",
    "#             negative_img_path = self.image_paths[negative_img_idx]\n",
    "#             negative_image = Image.open(negative_img_path).convert('RGB')\n",
    "#             negative_image = self.transform(negative_image)\n",
    "#         else:\n",
    "#             negative_img_idx = random.choice([i for i in range(len(self.image_paths)) if self.image_labels[i] != img_class])\n",
    "#             negative_img_path = self.image_paths[negative_img_idx]\n",
    "#             negative_image = Image.open(negative_img_path).convert('RGB')\n",
    "#             negative_image = self.transform(negative_image)\n",
    "        \n",
    "#         return eeg, image, negative_image, torch.LongTensor([img_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84323b38-c45e-4262-aa92-0239d87843dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eggimagedecode\n",
    "# class EEGImageDataset(Dataset):\n",
    "#     def __init__(self, eeg_file_path, images_dir, transform=None):\n",
    "#         data_dict = np.load(eeg_file_path, allow_pickle=True).item()\n",
    "#         self.eeg_data = data_dict['preprocessed_eeg_data']\n",
    "        \n",
    "#         self.images_dir = Path(images_dir)\n",
    "#         self.class_dirs = sorted([d for d in self.images_dir.iterdir() if d.is_dir()])\n",
    "#         self.class_to_idx = {d.name: i for i, d in enumerate(self.class_dirs)}\n",
    "        \n",
    "#         self.image_paths = []\n",
    "#         self.image_labels = []\n",
    "        \n",
    "#         for class_idx, class_dir in enumerate(self.class_dirs):\n",
    "#             image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
    "#             for img_path in image_files:\n",
    "#                 self.image_paths.append(img_path)\n",
    "#                 self.image_labels.append(class_idx)\n",
    "        \n",
    "#         self.class_to_images = {}\n",
    "#         for i, label in enumerate(self.image_labels):\n",
    "#             if label not in self.class_to_images:\n",
    "#                 self.class_to_images[label] = []\n",
    "#             self.class_to_images[label].append(i)\n",
    "        \n",
    "#         self.transform = transform or transforms.Compose([\n",
    "#             transforms.Resize((128, 128)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "#         ])\n",
    "        \n",
    "#         print(f\"Dataset: {len(self.eeg_data)} EEG samples, {len(self.image_paths)} images\")\n",
    "#         print(f\"Classes: {len(self.class_dirs)}\")\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return min(len(self.eeg_data), len(self.image_paths))\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         eeg = torch.FloatTensor(self.eeg_data[idx])\n",
    "        \n",
    "#         if idx < len(self.image_paths):\n",
    "#             img_idx = idx\n",
    "#         else:\n",
    "#             img_idx = random.randint(0, len(self.image_paths) - 1)\n",
    "            \n",
    "#         img_path = self.image_paths[img_idx]\n",
    "#         img_class = self.image_labels[img_idx]\n",
    "        \n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "        \n",
    "#         negative_class = random.choice([c for c in self.class_to_images.keys() if c != img_class])\n",
    "#         negative_img_idx = random.choice(self.class_to_images[negative_class])\n",
    "#         negative_img_path = self.image_paths[negative_img_idx]\n",
    "#         negative_image = Image.open(negative_img_path).convert('RGB')\n",
    "#         negative_image = self.transform(negative_image)\n",
    "        \n",
    "#         return eeg, image, negative_image, torch.LongTensor([img_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69b04f-2b4d-452c-9f19-13579a769429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(eeg_pickle_path):\n",
    "    with open(eeg_pickle_path, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    x_test = data_dict['x_test']  # (5706, 14, 32, 1)\n",
    "    y_test = data_dict['y_test']  # (5706, 10)\n",
    "    test_class_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    return x_test, test_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e4f03-70a1-47e4-9d74-f3ee90cfd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_hinge_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean(F.relu(1.0 - real_output))\n",
    "    fake_loss = torch.mean(F.relu(1.0 + fake_output))\n",
    "    return (real_loss + fake_loss) / 2.0\n",
    "\n",
    "def generator_hinge_loss(fake_output):\n",
    "    return -torch.mean(fake_output)\n",
    "\n",
    "def diff_augment(x, policy=\"color,translation\"):\n",
    "    if \"color\" in policy:\n",
    "        x = x + torch.randn_like(x) * 0.1\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "    \n",
    "    if \"translation\" in policy:\n",
    "        if random.random() > 0.5:\n",
    "            shift = random.randint(-4, 4)\n",
    "            x = torch.roll(x, shift, dims=2)\n",
    "            x = torch.roll(x, shift, dims=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36225b3-5ab7-4625-bbde-17432be393ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_augment_mnist(x, policy=\"color,translation\"):\n",
    "    if \"color\" in policy:\n",
    "        x = x + torch.randn_like(x) * 0.05\n",
    "        x = torch.clamp(x, -1, 1)\n",
    "    \n",
    "    if \"translation\" in policy:\n",
    "        if random.random() > 0.5:\n",
    "            shift = random.randint(-2, 2)  \n",
    "            x = torch.roll(x, shift, dims=2)\n",
    "            x = torch.roll(x, shift, dims=3)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6b41a-a143-4ed8-8366-8d888936b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(eeg_pickle_path, num_epochs=100, batch_size=32, lr=0.0002):\n",
    "    dataset = EEGMNISTDataset(eeg_pickle_path, train=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    eeg_encoder = EEGEncoder(input_channels=14, sequence_length=32).to(device)\n",
    "    image_encoder = ImageEncoder().to(device) \n",
    "    generator = Generator(n_classes=10).to(device) \n",
    "    discriminator = Discriminator(n_classes=10).to(device) \n",
    "    \n",
    "    contrastive_optimizer = optim.Adam(\n",
    "        list(eeg_encoder.parameters()) + list(image_encoder.parameters()), \n",
    "        lr=lr, betas=(0.9, 0.999)\n",
    "    )\n",
    "    g_optimizer = optim.Adam(\n",
    "        list(eeg_encoder.parameters()) + list(generator.parameters()), \n",
    "        lr=lr, betas=(0.0, 0.9)\n",
    "    )\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "    \n",
    "    contrastive_losses = []\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        contrastive_loss_total = 0\n",
    "        g_loss_total = 0\n",
    "        d_loss_total = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (eeg_batch, real_images, negative_images, class_labels) in enumerate(pbar):\n",
    "            batch_size_actual = eeg_batch.size(0)\n",
    "            \n",
    "            eeg_batch = eeg_batch.to(device)\n",
    "            real_images = real_images.to(device)\n",
    "            negative_images = negative_images.to(device)\n",
    "            class_labels = class_labels.squeeze().to(device)\n",
    "            \n",
    "            # =============== Contrastive Learning Phase ===============\n",
    "            contrastive_optimizer.zero_grad()\n",
    "            \n",
    "            _, eeg_contrastive_features = eeg_encoder(eeg_batch, return_contrastive=True)\n",
    "            img_contrastive_features = image_encoder(real_images)\n",
    "            negative_img_features = image_encoder(negative_images)\n",
    "            \n",
    "            info_nce = info_nce_loss(eeg_contrastive_features, img_contrastive_features)\n",
    "            triplet = triplet_loss(eeg_contrastive_features, img_contrastive_features, negative_img_features)\n",
    "            \n",
    "            contrastive_loss = info_nce + 0.5 * triplet\n",
    "            contrastive_loss.backward()\n",
    "            contrastive_optimizer.step()\n",
    "            \n",
    "            # =============== GAN Training Phase ===============\n",
    "            noise = torch.randn(batch_size_actual, 100).to(device)\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            eeg_features = eeg_encoder(eeg_batch).detach()\n",
    "            \n",
    "            fake_images = generator(eeg_features, noise, class_labels)\n",
    "            \n",
    "            real_images_aug = diff_augment_mnist(real_images)\n",
    "            fake_images_aug = diff_augment_mnist(fake_images.detach())\n",
    "            \n",
    "            real_output = discriminator(real_images_aug, class_labels)\n",
    "            fake_output = discriminator(fake_images_aug, class_labels)\n",
    "            \n",
    "            d_loss = discriminator_hinge_loss(real_output, fake_output)\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            noise2 = torch.randn(batch_size_actual, 100).to(device)\n",
    "            eeg_features = eeg_encoder(eeg_batch)\n",
    "            fake_images = generator(eeg_features, noise, class_labels)\n",
    "            fake_images2 = generator(eeg_features, noise2, class_labels)\n",
    "            \n",
    "            fake_images_aug = diff_augment_mnist(fake_images)\n",
    "            fake_images2_aug = diff_augment_mnist(fake_images2)\n",
    "            \n",
    "            fake_output = discriminator(fake_images_aug, class_labels)\n",
    "            fake_output2 = discriminator(fake_images2_aug, class_labels)\n",
    "            \n",
    "            g_loss = generator_hinge_loss(fake_output) + generator_hinge_loss(fake_output2)\n",
    "            \n",
    "            mode_loss = torch.mean(torch.abs(fake_images2 - fake_images)) / (\n",
    "                torch.mean(torch.abs(noise2 - noise)) + 1e-5)\n",
    "            mode_loss = 1.0 / (mode_loss + 1e-5)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                real_img_features = image_encoder(real_images)\n",
    "            fake_img_features = image_encoder(fake_images)\n",
    "            alignment_loss = F.mse_loss(fake_img_features, real_img_features)\n",
    "            \n",
    "            total_g_loss = g_loss + 1.0 * mode_loss + 0.1 * alignment_loss\n",
    "            total_g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            contrastive_loss_total += contrastive_loss.item()\n",
    "            g_loss_total += total_g_loss.item()\n",
    "            d_loss_total += d_loss.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Cont_Loss': f'{contrastive_loss_total/(batch_idx+1):.4f}',\n",
    "                'G_Loss': f'{g_loss_total/(batch_idx+1):.4f}',\n",
    "                'D_Loss': f'{d_loss_total/(batch_idx+1):.4f}'\n",
    "            })\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample_eeg = eeg_batch[:8]\n",
    "                sample_noise = torch.randn(8, 100).to(device)\n",
    "                sample_classes = class_labels[:8]\n",
    "                sample_features = eeg_encoder(sample_eeg)\n",
    "                sample_images = generator(sample_features, sample_noise, sample_classes)\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                for i in range(8):\n",
    "                    plt.subplot(2, 4, i+1)\n",
    "                    img = sample_images[i].cpu().squeeze() \n",
    "                    img = (img + 1) / 2  \n",
    "                    img = torch.clamp(img, 0, 1)\n",
    "                    plt.imshow(img, cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f'Class: {sample_classes[i].item()}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'mnist_generated_samples_epoch_{epoch+1}.png')\n",
    "                plt.close()\n",
    "        \n",
    "        contrastive_losses.append(contrastive_loss_total/len(dataloader))\n",
    "        g_losses.append(g_loss_total/len(dataloader))\n",
    "        d_losses.append(d_loss_total/len(dataloader))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Contrastive_Loss = {contrastive_loss_total/len(dataloader):.4f}, '\n",
    "              f'G_Loss = {g_loss_total/len(dataloader):.4f}, '\n",
    "              f'D_Loss = {d_loss_total/len(dataloader):.4f}')\n",
    "    \n",
    "    plot_training_history_with_contrastive(contrastive_losses, g_losses, d_losses)\n",
    "    \n",
    "    return eeg_encoder, image_encoder, generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28beb7dc-2565-4dc5-90a8-7257c5a24914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtviz ImageNet\n",
    "\n",
    "# def train2(eeg_pickle_path, images_dir, num_epochs=100, batch_size=32, lr=0.0002):\n",
    "#     dataset = EEGImageDataset(eeg_pickle_path, images_dir)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "#     eeg_encoder = EEGEncoder(input_channels=14, sequence_length=32).to(device)\n",
    "#     image_encoder = ImageEncoder().to(device)\n",
    "#     generator = Generator(n_classes=10).to(device)  \n",
    "#     discriminator = Discriminator(n_classes=10).to(device) \n",
    "    \n",
    "#     contrastive_optimizer = optim.Adam(\n",
    "#         list(eeg_encoder.parameters()) + list(image_encoder.parameters()), \n",
    "#         lr=lr, betas=(0.9, 0.999)\n",
    "#     )\n",
    "#     g_optimizer = optim.Adam(\n",
    "#         list(eeg_encoder.parameters()) + list(generator.parameters()), \n",
    "#         lr=lr, betas=(0.0, 0.9)\n",
    "#     )\n",
    "#     d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "    \n",
    "#     contrastive_losses = []\n",
    "#     g_losses = []\n",
    "#     d_losses = []\n",
    "    \n",
    "#     print(\"Starting training...\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         contrastive_loss_total = 0\n",
    "#         g_loss_total = 0\n",
    "#         d_loss_total = 0\n",
    "        \n",
    "#         pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "#         for batch_idx, (eeg_batch, real_images, negative_images, class_labels) in enumerate(pbar):\n",
    "#             batch_size_actual = eeg_batch.size(0)\n",
    "            \n",
    "#             eeg_batch = eeg_batch.to(device)\n",
    "#             real_images = real_images.to(device)\n",
    "#             negative_images = negative_images.to(device)\n",
    "#             class_labels = class_labels.squeeze().to(device)\n",
    "            \n",
    "#             # =============== Contrastive Learning Phase ===============\n",
    "#             contrastive_optimizer.zero_grad()\n",
    "            \n",
    "#             _, eeg_contrastive_features = eeg_encoder(eeg_batch, return_contrastive=True)\n",
    "#             img_contrastive_features = image_encoder(real_images)\n",
    "#             negative_img_features = image_encoder(negative_images)\n",
    "            \n",
    "#             info_nce = info_nce_loss(eeg_contrastive_features, img_contrastive_features)\n",
    "#             triplet = triplet_loss(eeg_contrastive_features, img_contrastive_features, negative_img_features)\n",
    "            \n",
    "#             contrastive_loss = info_nce + 0.5 * triplet\n",
    "#             contrastive_loss.backward()\n",
    "#             contrastive_optimizer.step()\n",
    "            \n",
    "#             # =============== GAN Training Phase ===============\n",
    "#             noise = torch.randn(batch_size_actual, 100).to(device)\n",
    "            \n",
    "#             d_optimizer.zero_grad()\n",
    "            \n",
    "#             eeg_features = eeg_encoder(eeg_batch).detach()\n",
    "            \n",
    "#             fake_images = generator(eeg_features, noise, class_labels)\n",
    "            \n",
    "#             real_images_aug = diff_augment(real_images)\n",
    "#             fake_images_aug = diff_augment(fake_images.detach())\n",
    "            \n",
    "#             real_output = discriminator(real_images_aug, class_labels)\n",
    "#             fake_output = discriminator(fake_images_aug, class_labels)\n",
    "            \n",
    "#             d_loss = discriminator_hinge_loss(real_output, fake_output)\n",
    "#             d_loss.backward()\n",
    "#             d_optimizer.step()\n",
    "            \n",
    "#             g_optimizer.zero_grad()\n",
    "            \n",
    "#             noise2 = torch.randn(batch_size_actual, 100).to(device)\n",
    "#             eeg_features = eeg_encoder(eeg_batch)\n",
    "#             fake_images = generator(eeg_features, noise, class_labels)\n",
    "#             fake_images2 = generator(eeg_features, noise2, class_labels)\n",
    "            \n",
    "#             fake_images_aug = diff_augment(fake_images)\n",
    "#             fake_images2_aug = diff_augment(fake_images2)\n",
    "            \n",
    "#             fake_output = discriminator(fake_images_aug, class_labels)\n",
    "#             fake_output2 = discriminator(fake_images2_aug, class_labels)\n",
    "            \n",
    "#             g_loss = generator_hinge_loss(fake_output) + generator_hinge_loss(fake_output2)\n",
    "            \n",
    "#             mode_loss = torch.mean(torch.abs(fake_images2 - fake_images)) / (\n",
    "#                 torch.mean(torch.abs(noise2 - noise)) + 1e-5)\n",
    "#             mode_loss = 1.0 / (mode_loss + 1e-5)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 real_img_features = image_encoder(real_images)\n",
    "#             fake_img_features = image_encoder(fake_images)\n",
    "#             alignment_loss = F.mse_loss(fake_img_features, real_img_features)\n",
    "            \n",
    "#             total_g_loss = g_loss + 1.0 * mode_loss + 0.1 * alignment_loss\n",
    "#             total_g_loss.backward()\n",
    "#             g_optimizer.step()\n",
    "            \n",
    "#             contrastive_loss_total += contrastive_loss.item()\n",
    "#             g_loss_total += total_g_loss.item()\n",
    "#             d_loss_total += d_loss.item()\n",
    "            \n",
    "#             pbar.set_postfix({\n",
    "#                 'Cont_Loss': f'{contrastive_loss_total/(batch_idx+1):.4f}',\n",
    "#                 'G_Loss': f'{g_loss_total/(batch_idx+1):.4f}',\n",
    "#                 'D_Loss': f'{d_loss_total/(batch_idx+1):.4f}'\n",
    "#             })\n",
    "        \n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 sample_eeg = eeg_batch[:8]\n",
    "#                 sample_noise = torch.randn(8, 100).to(device)\n",
    "#                 sample_classes = class_labels[:8]\n",
    "#                 sample_features = eeg_encoder(sample_eeg)\n",
    "#                 sample_images = generator(sample_features, sample_noise, sample_classes)\n",
    "                \n",
    "#                 plt.figure(figsize=(12, 8))\n",
    "#                 for i in range(8):\n",
    "#                     plt.subplot(2, 4, i+1)\n",
    "#                     img = sample_images[i].cpu()\n",
    "#                     img = (img + 1) / 2  \n",
    "#                     img = torch.clamp(img, 0, 1)\n",
    "#                     plt.imshow(img.permute(1, 2, 0))\n",
    "#                     plt.axis('off')\n",
    "#                     plt.title(f'Class: {sample_classes[i].item()}')\n",
    "                \n",
    "#                 plt.tight_layout()\n",
    "#                 plt.savefig(f'generated_samples_epoch_{epoch+1}.png')\n",
    "#                 plt.close()\n",
    "        \n",
    "#         contrastive_losses.append(contrastive_loss_total/len(dataloader))\n",
    "#         g_losses.append(g_loss_total/len(dataloader))\n",
    "#         d_losses.append(d_loss_total/len(dataloader))\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}: Contrastive_Loss = {contrastive_loss_total/len(dataloader):.4f}, '\n",
    "#               f'G_Loss = {g_loss_total/len(dataloader):.4f}, '\n",
    "#               f'D_Loss = {d_loss_total/len(dataloader):.4f}')\n",
    "    \n",
    "#     plot_training_history_with_contrastive(contrastive_losses, g_losses, d_losses)\n",
    "    \n",
    "#     return eeg_encoder, image_encoder, generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e37bf8-3023-4e4d-b4d0-aba33dd0bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eggimagedecode\n",
    "# def train3(eeg_file_path, images_dir, num_epochs=100, batch_size=32, lr=0.0002):\n",
    "#     dataset = EEGImageDataset(eeg_file_path, images_dir)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "#     eeg_encoder = EEGEncoder().to(device)\n",
    "#     image_encoder = ImageEncoder().to(device)\n",
    "#     generator = Generator(n_classes=len(dataset.class_dirs)).to(device)\n",
    "#     discriminator = Discriminator(n_classes=len(dataset.class_dirs)).to(device)\n",
    "    \n",
    "#     contrastive_optimizer = optim.Adam(\n",
    "#         list(eeg_encoder.parameters()) + list(image_encoder.parameters()), \n",
    "#         lr=lr, betas=(0.9, 0.999)\n",
    "#     )\n",
    "#     g_optimizer = optim.Adam(\n",
    "#         list(eeg_encoder.parameters()) + list(generator.parameters()), \n",
    "#         lr=lr, betas=(0.0, 0.9)\n",
    "#     )\n",
    "#     d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "    \n",
    "#     contrastive_losses = []\n",
    "#     g_losses = []\n",
    "#     d_losses = []\n",
    "    \n",
    "#     print(\"Starting training...\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         contrastive_loss_total = 0\n",
    "#         g_loss_total = 0\n",
    "#         d_loss_total = 0\n",
    "        \n",
    "#         pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "#         for batch_idx, (eeg_batch, real_images, negative_images, class_labels) in enumerate(pbar):\n",
    "#             batch_size_actual = eeg_batch.size(0)\n",
    "            \n",
    "#             eeg_batch = eeg_batch.to(device)\n",
    "#             real_images = real_images.to(device)\n",
    "#             negative_images = negative_images.to(device)\n",
    "#             class_labels = class_labels.squeeze().to(device)\n",
    "            \n",
    "#             # =============== Contrastive Learning Phase ===============\n",
    "#             contrastive_optimizer.zero_grad()\n",
    "            \n",
    "#             _, eeg_contrastive_features = eeg_encoder(eeg_batch, return_contrastive=True)\n",
    "#             img_contrastive_features = image_encoder(real_images)\n",
    "#             negative_img_features = image_encoder(negative_images)\n",
    "            \n",
    "#             info_nce = info_nce_loss(eeg_contrastive_features, img_contrastive_features)\n",
    "#             triplet = triplet_loss(eeg_contrastive_features, img_contrastive_features, negative_img_features)\n",
    "            \n",
    "#             contrastive_loss = info_nce + 0.5 * triplet\n",
    "#             contrastive_loss.backward()\n",
    "#             contrastive_optimizer.step()\n",
    "            \n",
    "#             # =============== GAN Training Phase ===============\n",
    "#             noise = torch.randn(batch_size_actual, 100).to(device)\n",
    "            \n",
    "#             d_optimizer.zero_grad()\n",
    "#             eeg_features = eeg_encoder(eeg_batch).detach()\n",
    "            \n",
    "#             fake_images = generator(eeg_features, noise, class_labels)\n",
    "            \n",
    "#             real_images_aug = diff_augment(real_images)\n",
    "#             fake_images_aug = diff_augment(fake_images.detach())\n",
    "            \n",
    "#             real_output = discriminator(real_images_aug, class_labels)\n",
    "#             fake_output = discriminator(fake_images_aug, class_labels)\n",
    "            \n",
    "#             d_loss = discriminator_hinge_loss(real_output, fake_output)\n",
    "#             d_loss.backward()\n",
    "#             d_optimizer.step()\n",
    "            \n",
    "#             g_optimizer.zero_grad()\n",
    "            \n",
    "#             noise2 = torch.randn(batch_size_actual, 100).to(device)\n",
    "#             eeg_features = eeg_encoder(eeg_batch)\n",
    "#             fake_images = generator(eeg_features, noise, class_labels)\n",
    "#             fake_images2 = generator(eeg_features, noise2, class_labels)\n",
    "            \n",
    "#             fake_images_aug = diff_augment(fake_images)\n",
    "#             fake_images2_aug = diff_augment(fake_images2)\n",
    "            \n",
    "#             fake_output = discriminator(fake_images_aug, class_labels)\n",
    "#             fake_output2 = discriminator(fake_images2_aug, class_labels)\n",
    "            \n",
    "#             g_loss = generator_hinge_loss(fake_output) + generator_hinge_loss(fake_output2)\n",
    "            \n",
    "#             mode_loss = torch.mean(torch.abs(fake_images2 - fake_images)) / (\n",
    "#                 torch.mean(torch.abs(noise2 - noise)) + 1e-5)\n",
    "#             mode_loss = 1.0 / (mode_loss + 1e-5)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 real_img_features = image_encoder(real_images)\n",
    "#             fake_img_features = image_encoder(fake_images)\n",
    "#             alignment_loss = F.mse_loss(fake_img_features, real_img_features)\n",
    "            \n",
    "#             total_g_loss = g_loss + 1.0 * mode_loss + 0.1 * alignment_loss\n",
    "#             total_g_loss.backward()\n",
    "#             g_optimizer.step()\n",
    "            \n",
    "#             contrastive_loss_total += contrastive_loss.item()\n",
    "#             g_loss_total += total_g_loss.item()\n",
    "#             d_loss_total += d_loss.item()\n",
    "            \n",
    "#             pbar.set_postfix({\n",
    "#                 'Cont_Loss': f'{contrastive_loss_total/(batch_idx+1):.4f}',\n",
    "#                 'G_Loss': f'{g_loss_total/(batch_idx+1):.4f}',\n",
    "#                 'D_Loss': f'{d_loss_total/(batch_idx+1):.4f}'\n",
    "#             })\n",
    "        \n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 sample_eeg = eeg_batch[:8]\n",
    "#                 sample_noise = torch.randn(8, 100).to(device)\n",
    "#                 sample_classes = class_labels[:8]\n",
    "#                 sample_features = eeg_encoder(sample_eeg)\n",
    "#                 sample_images = generator(sample_features, sample_noise, sample_classes)\n",
    "                \n",
    "#                 plt.figure(figsize=(12, 8))\n",
    "#                 for i in range(8):\n",
    "#                     plt.subplot(2, 4, i+1)\n",
    "#                     img = sample_images[i].cpu()\n",
    "#                     img = (img + 1) / 2 \n",
    "#                     img = torch.clamp(img, 0, 1)\n",
    "#                     plt.imshow(img.permute(1, 2, 0))\n",
    "#                     plt.axis('off')\n",
    "#                     plt.title(f'Class: {sample_classes[i].item()}')\n",
    "                \n",
    "#                 plt.tight_layout()\n",
    "#                 plt.savefig(f'generated_samples_epoch_{epoch+1}.png')\n",
    "#                 plt.close()\n",
    "        \n",
    "#         contrastive_losses.append(contrastive_loss_total/len(dataloader))\n",
    "#         g_losses.append(g_loss_total/len(dataloader))\n",
    "#         d_losses.append(d_loss_total/len(dataloader))\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}: Contrastive_Loss = {contrastive_loss_total/len(dataloader):.4f}, '\n",
    "#               f'G_Loss = {g_loss_total/len(dataloader):.4f}, '\n",
    "#               f'D_Loss = {d_loss_total/len(dataloader):.4f}')\n",
    "    \n",
    "#     plot_training_history_with_contrastive(contrastive_losses, g_losses, d_losses)\n",
    "    \n",
    "#     return eeg_encoder, image_encoder, generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9e375-9f9e-4511-9a92-db806eb72f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(eeg_encoder, generator, dataset, num_samples=8, save_path='mnist_generated_samples.png'):\n",
    "    eeg_encoder.eval()\n",
    "    generator.eval()\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(20, 12))\n",
    "    fig.suptitle('EEG-to-MNIST Generation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            eeg_sample, real_image, _, class_label = dataset[idx]\n",
    "            \n",
    "            eeg_batch = eeg_sample.unsqueeze(0).to(device)\n",
    "            class_batch = class_label.to(device)\n",
    "            noise = torch.randn(1, 100).to(device)\n",
    "            \n",
    "            eeg_features = eeg_encoder(eeg_batch)\n",
    "            generated_image = generator(eeg_features, noise, class_batch)\n",
    "            \n",
    "            real_img = (real_image.squeeze() + 1) / 2  \n",
    "            real_img = torch.clamp(real_img, 0, 1)\n",
    "            \n",
    "            gen_img = (generated_image.squeeze().cpu() + 1) / 2\n",
    "            gen_img = torch.clamp(gen_img, 0, 1)\n",
    "            \n",
    "            eeg_plot = eeg_sample.mean(dim=0).squeeze().numpy()\n",
    "            \n",
    "            axes[0, i].plot(eeg_plot)\n",
    "            axes[0, i].set_title(f'EEG Signal\\nClass: {class_label.item()}', fontsize=10)\n",
    "            axes[0, i].set_xlabel('Time')\n",
    "            axes[0, i].set_ylabel('Amplitude')\n",
    "            axes[0, i].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, i].imshow(real_img, cmap='gray')\n",
    "            axes[1, i].set_title('Real MNIST', fontsize=10)\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            axes[2, i].imshow(gen_img, cmap='gray')\n",
    "            axes[2, i].set_title('Generated', fontsize=10)\n",
    "            axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"MNIST visualization saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904471a7-f716-4b25-b4d0-319331df885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_generated_samples(eeg_encoder, generator, dataset, num_samples=8, save_path='generated_samples.png'):\n",
    "#     eeg_encoder.eval()\n",
    "#     generator.eval()\n",
    "    \n",
    "#     indices = random.sample(range(len(dataset)), num_samples)\n",
    "    \n",
    "#     fig, axes = plt.subplots(3, num_samples, figsize=(20, 12))\n",
    "#     fig.suptitle('EEG-to-Image Generation Results with Contrastive Learning', fontsize=16, fontweight='bold')\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i, idx in enumerate(indices):\n",
    "#             eeg_sample, real_image, _, class_label = dataset[idx]\n",
    "            \n",
    "#             eeg_batch = eeg_sample.unsqueeze(0).to(device)\n",
    "#             class_batch = class_label.to(device)\n",
    "#             noise = torch.randn(1, 100).to(device)\n",
    "            \n",
    "#             eeg_features = eeg_encoder(eeg_batch)\n",
    "#             generated_image = generator(eeg_features, noise, class_batch)\n",
    "            \n",
    "#             real_img = (real_image + 1) / 2 \n",
    "#             real_img = torch.clamp(real_img, 0, 1)\n",
    "            \n",
    "#             gen_img = (generated_image.squeeze().cpu() + 1) / 2\n",
    "#             gen_img = torch.clamp(gen_img, 0, 1)\n",
    "            \n",
    "#             eeg_plot = eeg_sample.mean(dim=0).mean(dim=0).numpy()  \n",
    "            \n",
    "#             axes[0, i].plot(eeg_plot)\n",
    "#             axes[0, i].set_title(f'EEG Signal\\nClass: {class_label.item()}', fontsize=10)\n",
    "#             axes[0, i].set_xlabel('Time')\n",
    "#             axes[0, i].set_ylabel('Amplitude')\n",
    "#             axes[0, i].grid(True, alpha=0.3)\n",
    "            \n",
    "#             axes[1, i].imshow(real_img.permute(1, 2, 0))\n",
    "#             axes[1, i].set_title('Real Image', fontsize=10)\n",
    "#             axes[1, i].axis('off')\n",
    "            \n",
    "#             axes[2, i].imshow(gen_img.permute(1, 2, 0))\n",
    "#             axes[2, i].set_title('Generated Image', fontsize=10)\n",
    "#             axes[2, i].axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(f\"Visualization saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921792b-0020-482d-8bf1-b4ec9792a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history_with_contrastive(contrastive_losses, g_losses, d_losses, save_path='training_history.png'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(contrastive_losses, label='Contrastive Loss', color='green')\n",
    "    plt.title('Contrastive Learning Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(g_losses, label='Generator Loss', color='blue')\n",
    "    plt.plot(d_losses, label='Discriminator Loss', color='red')\n",
    "    plt.title('GAN Training Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    window = max(1, len(g_losses) // 20)\n",
    "    cont_smooth = np.convolve(contrastive_losses, np.ones(window)/window, mode='valid')\n",
    "    g_smooth = np.convolve(g_losses, np.ones(window)/window, mode='valid')\n",
    "    d_smooth = np.convolve(d_losses, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    plt.plot(cont_smooth, label='Contrastive Loss (Smoothed)', color='green')\n",
    "    plt.plot(g_smooth, label='Generator Loss (Smoothed)', color='blue')\n",
    "    plt.plot(d_smooth, label='Discriminator Loss (Smoothed)', color='red')\n",
    "    plt.title('Smoothed Training Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training history saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f8540-a970-4d89-afea-fc54031296f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contrastive_alignment(eeg_encoder, image_encoder, dataset, num_samples=100):\n",
    "    eeg_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            eeg_sample, real_image, _, class_label = dataset[i]\n",
    "            \n",
    "            eeg_batch = eeg_sample.unsqueeze(0).to(device)\n",
    "            img_batch = real_image.unsqueeze(0).to(device)\n",
    "            \n",
    "            _, eeg_features = eeg_encoder(eeg_batch, return_contrastive=True)\n",
    "            img_features = image_encoder(img_batch)\n",
    "            \n",
    "            similarity = F.cosine_similarity(eeg_features, img_features, dim=1)\n",
    "            similarities.append(similarity.item())\n",
    "    \n",
    "    avg_similarity = np.mean(similarities)\n",
    "    std_similarity = np.std(similarities)\n",
    "    \n",
    "    print(f\"Average EEG-Image similarity: {avg_similarity:.4f} ± {std_similarity:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(similarities, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(avg_similarity, color='red', linestyle='--', \n",
    "                label=f'Mean: {avg_similarity:.4f}')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of EEG-Image Feature Similarities')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('similarity_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_similarity, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a867a",
   "metadata": {
    "papermill": {
     "duration": 2074.143719,
     "end_time": "2025-06-18T05:13:41.661957",
     "exception": true,
     "start_time": "2025-06-18T04:39:07.518238",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eeg_pickle_path = \"/kaggle/input/thoughtviz/data/data/eeg/digit/data.pkl\"\n",
    "# images_dir = \"/kaggle/input/thoughtviz/images/images/ImageNet-Filtered\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e4ea0-a09d-40f6-afb7-4f20f075f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_encoder, image_encoder, generator, discriminator = train(\n",
    "    eeg_pickle_path=eeg_pickle_path,\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    lr=0.0002\n",
    ")\n",
    "# eeg_encoder, image_encoder, generator, discriminator = train2(\n",
    "#         eeg_pickle_path=eeg_pickle_path,\n",
    "#         images_dir=images_dir,\n",
    "#         num_epochs=10,\n",
    "#         batch_size=32,\n",
    "#         lr=0.0002\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9438cf-b477-470f-b42d-1fcb0021bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGMNISTDataset(eeg_pickle_path, train=True)\n",
    "# dataset = EEGImageDataset(eeg_pickle_path, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc3891-a343-4a80-8a29-8355b86725f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(eeg_encoder, generator, dataset, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac200a0-8662-4be3-a446-5f0725822662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_file_path = \"/kaggle/input/dongyangli-deleeg-image-decode/sub-01/sub-01/preprocessed_eeg_training.npy\"\n",
    "# images_dir = \"/kaggle/input/dongyangli-deleeg-image-decode/osfstorage-archive/training_images/training_images\"\n",
    "\n",
    "# print(\"Training EEG-to-Image GAN with Contrastive Learning...\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# eeg_encoder, image_encoder, generator, discriminator = train_gan_with_contrastive(\n",
    "#     eeg_file_path=eeg_file_path,\n",
    "#     images_dir=images_dir,\n",
    "#     num_epochs=80,  \n",
    "#     batch_size=64, \n",
    "#     lr=0.0002\n",
    "# )\n",
    "\n",
    "# torch.save({\n",
    "#     'eeg_encoder': eeg_encoder.state_dict(),\n",
    "#     'image_encoder': image_encoder.state_dict(),\n",
    "#     'generator': generator.state_dict(),\n",
    "#     'discriminator': discriminator.state_dict()\n",
    "# }, 'eeg_to_image_gan_contrastive.pth')\n",
    "\n",
    "# print(\"\\nTraining completed and models saved!\")\n",
    "\n",
    "# dataset = EEGImageDataset(eeg_file_path, images_dir)\n",
    "\n",
    "# print(\"\\nEvaluating EEG-Image feature alignment...\")\n",
    "# avg_similarity, similarities = evaluate_contrastive_alignment(\n",
    "#     eeg_encoder, image_encoder, dataset, num_samples=200\n",
    "# )\n",
    "\n",
    "# print(\"\\nGenerating visualizations...\")\n",
    "# visualize_samples(eeg_encoder, generator, dataset, num_samples=8)\n",
    "\n",
    "# print(\"All tasks completed successfully!\")\n",
    "# print(f\"Average feature alignment score: {avg_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7627685,
     "sourceId": 12114795,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2091.76646,
   "end_time": "2025-06-18T05:13:46.274887",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-18T04:38:54.508427",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
